{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2c73fa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense , Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "90c54564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines used :  40000\n",
      "Example line : resolved. resolved.\n"
     ]
    }
   ],
   "source": [
    "#  LOAD SHAKESPEARE TEXT\n",
    "# This downloads a text file with Shakespeare's works\n",
    "\n",
    "path = tf.keras.utils.get_file(\n",
    "    \"shakespeare.txt\" , \n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
    ")\n",
    "\n",
    "\n",
    "# Read the text file into a big string\n",
    "\n",
    "text = open(path , \"r\" , encoding = \"utf-8\").read().lower()\n",
    "\n",
    "# Split the text into lines (each line is like a sentence)\n",
    "# NOTE: use \"\\n\" (newline) not \"/n\"\n",
    "corpus = text.split(\"\\n\")\n",
    "\n",
    "# To make it simple and fast, we only use the first 40000 lines\n",
    "corpus = corpus[:40000]\n",
    "\n",
    "print(\"Number of lines used : \"  , len(corpus))\n",
    "print(\"Example line :\" , corpus[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1aa49ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZE THE TEXT (TURN WORDS INTO NUMBERS)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)   #learn the word --> number mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "40d6b302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 204089\n"
     ]
    }
   ],
   "source": [
    "# Convert entire text to a single list of token IDs\n",
    "tokens = tokenizer.texts_to_sequences([text])[0]\n",
    "print(\"Total tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9432bba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size 12633\n"
     ]
    }
   ],
   "source": [
    "# Total number of different words in our data (vocabulary size)\n",
    "\n",
    "total_words = len(tokenizer.word_index) +1  #+1 for padding index\n",
    "print(\"vocabulary size\" , total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "da707b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reverse dictionary (ID → word)\n",
    "reverse_word_index = {idx: word for word, idx in tokenizer.word_index.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e28e2d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 204029\n"
     ]
    }
   ],
   "source": [
    "# CREATE FIXED-LENGTH TRAINING SEQUENCES\n",
    "# We choose a sequence length (number of words in each training example).\n",
    "sequence_length = 60 \n",
    "\n",
    "input_sequences = []\n",
    "\n",
    "# We slide a window of length (sequence_length + 1) across the tokens.\n",
    "for i in range(sequence_length, len(tokens)):\n",
    "    seq = tokens[i - sequence_length : i + 1]\n",
    "    input_sequences.append(seq)\n",
    "\n",
    "print(\"Number of sequences:\", len(input_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c9dc66c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (204029, 60)\n",
      "y shape: (204029, 12633)\n"
     ]
    }
   ],
   "source": [
    "# PREPARE X AND y\n",
    "input_sequences = np.array(input_sequences)\n",
    "\n",
    "# All but last token → input\n",
    "X = input_sequences[:, :-1]\n",
    "# Last token → target\n",
    "y = input_sequences[:, -1]\n",
    "\n",
    "# One-hot encode target\n",
    "y = to_categorical(y, num_classes=total_words)\n",
    "\n",
    "print(\"X shape:\", X.shape)  # (num_samples, sequence_length)\n",
    "print(\"y shape:\", y.shape)  # (num_samples, vocab_size)# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8b98ffaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 60, 256)           3234048   \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 60, 256)           525312    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 60, 256)           0         \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 12633)             3246681   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,531,353\n",
      "Trainable params: 7,531,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# BUILD LSTM MODEL\n",
    "total_words = y.shape[1]   #vocab size from y shape\n",
    "model = Sequential()             # create model\n",
    "model.add(                          # add layers\n",
    "    Embedding(\n",
    "        input_dim=total_words,      # vocab size\n",
    "        output_dim=256,             # embedding size (can tune)\n",
    "        input_length=X.shape[1]     # sequence length \n",
    "    )\n",
    ")\n",
    "model.add(LSTM(256 , return_sequences=True))  \n",
    "model.add(Dropout(0.2)) \n",
    "\n",
    "model.add(LSTM(256))             # bigger LSTM for more power\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(total_words, activation='softmax'))     # softmax output layer , Dense layer with vocab size outputs\n",
    "\n",
    "opt = Adam(learning_rate=0.001)          # Adam optimizer\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07b68e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1435/1435 [==============================] - ETA: 0s - loss: 6.8603 - accuracy: 0.0345\n",
      "Epoch 1: val_accuracy improved from -inf to 0.04097, saving model to best_shakespeare_model.h5\n",
      "1435/1435 [==============================] - 1064s 738ms/step - loss: 6.8603 - accuracy: 0.0345 - val_loss: 6.8641 - val_accuracy: 0.0410\n",
      "Epoch 2/40\n",
      "1435/1435 [==============================] - ETA: 0s - loss: 6.3593 - accuracy: 0.0630\n",
      "Epoch 2: val_accuracy improved from 0.04097 to 0.07161, saving model to best_shakespeare_model.h5\n",
      "1435/1435 [==============================] - 1107s 771ms/step - loss: 6.3593 - accuracy: 0.0630 - val_loss: 6.6949 - val_accuracy: 0.0716\n",
      "Epoch 3/40\n",
      "1435/1435 [==============================] - ETA: 0s - loss: 6.0683 - accuracy: 0.0840\n",
      "Epoch 3: val_accuracy improved from 0.07161 to 0.07656, saving model to best_shakespeare_model.h5\n",
      "1435/1435 [==============================] - 998s 696ms/step - loss: 6.0683 - accuracy: 0.0840 - val_loss: 6.6635 - val_accuracy: 0.0766\n",
      "Epoch 4/40\n",
      "1435/1435 [==============================] - ETA: 0s - loss: 5.8602 - accuracy: 0.0927"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint       # callbacks for training , modelcheckpoint is used to save the best model during training\n",
    "\n",
    "early_stop = EarlyStopping(        # stop training if no improvement\n",
    "    monitor='val_accuracy',\n",
    "    patience=3,          # stop if no improvement for 3 epochs\n",
    "    restore_best_weights=True     # restore best weights after stopping\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(       # save the best model during training\n",
    "    'best_shakespeare_model.h5',      \n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=40,           # try up to 40, EarlyStopping will stop earlier\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,    # use 10% of data for validation\n",
    "    callbacks=[early_stop, checkpoint],    # callbacks for training\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPERATURE SAMPLING FUNCTION\n",
    "\n",
    "def sample_with_temperature(preds, temperature=1.0):        # temperature sampling\n",
    "    \"\"\"\n",
    "    Sample an index from the predicted probabilities using temperature.\n",
    "    Lower temperature = more predictable.\n",
    "    Higher temperature = more random.\n",
    "    \"\"\"\n",
    "    preds = np.asarray(preds).astype(\"float64\")       # convert to numpy array\n",
    "     #we add 1e-8? Because sometimes a predicted probability can be 0,\n",
    "    # This makes sure nothing is exactly zero, but it’s so small that it doesn’t change the probabilities in any meaningful way.\n",
    "    preds = np.log(preds + 1e-8) / temperature     \n",
    "    \n",
    "    exp_preds = np.exp(preds)                                # exponentiate the log probabilities\n",
    "    preds = exp_preds / np.sum(exp_preds)                    # to get a valid probability distribution\n",
    "    return np.random.choice(len(preds), p=preds)             # to sample a word index according to this distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6e74fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT GENERATION FUNCTION\n",
    "\n",
    "def generate_text(seed_text, next_words=20, temperature=0.3):\n",
    "    \"\"\"\n",
    "    Generate new text starting from seed_text by predicting\n",
    "    next_words one by one with temperature sampling.\n",
    "    \"\"\"\n",
    "    text = seed_text\n",
    "\n",
    "    for _ in range(next_words):\n",
    "        # Convert current text to token IDs\n",
    "        token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "        # If no known tokens, stop\n",
    "        if len(token_list) == 0:\n",
    "            break\n",
    "\n",
    "        # Only keep the last sequence_length tokens\n",
    "        # (otherwise the input grows forever)\n",
    "        token_list = token_list[-X.shape[1]:]\n",
    "\n",
    "        # Pad to the fixed length needed by the model\n",
    "        token_list_padded = pad_sequences(\n",
    "            [token_list],\n",
    "            maxlen=X.shape[1],\n",
    "            padding='pre'\n",
    "        )\n",
    "\n",
    "        # Predict probabilities for next word\n",
    "        predicted_probs = model.predict(token_list_padded, verbose=0)[0]\n",
    "\n",
    "        # Sample next word index with temperature\n",
    "        predicted_id = np.argmax(predicted_probs)           # choose the word with highest probability\n",
    "\n",
    "        # Ignore padding index\n",
    "        if predicted_id == 0:\n",
    "            continue\n",
    "\n",
    "        # Get word from index\n",
    "        next_word = reverse_word_index.get(predicted_id, None)\n",
    "        if next_word is None:\n",
    "            break\n",
    "\n",
    "        # Append word to text\n",
    "        text += \" \" + next_word\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d8929911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or pleasant for it is being done what will you have but kneels and kiss me with a low gold i am a subject will have been more deep a young one though when we lay by the wall as if\n",
      "the king is dead ' and while he be sure i will not kill this world and break the english by my state and he to spend the world as much sway when this was between that\n",
      "love is set down for being done for shame rule my hearth but with some little train 'gainst my friends good my lord my lord and leave you lords to meet you\n"
     ]
    }
   ],
   "source": [
    "# TEST TEXT GENERATION\n",
    "\n",
    "print(generate_text(\"to be or\", next_words= 40, temperature=0.3))\n",
    "print(generate_text(\"the king\", next_words=35, temperature=0.2))\n",
    "print(generate_text(\"love is\", next_words= 30, temperature=0.1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ml-ai",
   "language": "python",
   "name": "conda-env-anaconda-ml-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
